{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train/dev/test data and bias and variances\n",
    "to recognize the bias and variances\n",
    "<img src = \"bias and variance.PNG\" width = 40%>\n",
    "### 2. L2 regulazation\n",
    "$||W||_1 = \\sum\\limits_{i = 1}^{m}|W|$  \n",
    "$||W||_2 = \\sum\\limits_{i=1}^{m}W^2$\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "if J can be defined like the top, then  \n",
    "$dW^{[L]} = 1/mdZ^{[L]}{dA^{[L-1]}}^T+\\frac{\\lambda}{m}W^{[L]}$  \n",
    "$W^{[L]} = (1-\\frac{\\lambda}{m})*W^{[L]}-(back propagation)$  \n",
    "so $W^{[L]}$ will decrease and the model will become simplier\n",
    "### 3. why the L2 regulazation can decrease the variance?\n",
    "the explaination1： if $\\lambda$ is enough big, many node will dispear\n",
    "<img src = \"regulazation_reason.PNG\" width = 40%>\n",
    "the explaination2: if $W^{[L]}$ can be small ,then the Z and A can be small ,which will drop in the linear region of the activation function  \n",
    "<img src = \"explaination2.PNG\" width = \"40%\">\n",
    "### 4. dropout regulazation \n",
    "\n",
    "no dropout at test time\n",
    "try to keep the input layer $keep-prop = 1$  \n",
    "if the layer you think has to much node, then use the dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.data normalization\n",
    "\n",
    "\n",
    "### 2. weight initialization\n",
    "\n",
    "can partially solve the vanishing and exploding gradient\n",
    "### 3. gradient checking -only use in debug! not in training - a time-consuming process\n",
    "### don't use dropout when using grad decend\n",
    "### remember regularization\n",
    "\n",
    "doing both sides of derivative gets more accurancy result than the single side \n",
    "<img>\n",
    "\n",
    "gradient checking implement：  \n",
    "all the $W^{[1]} b^{[1]}....W^{[L]} b^{[L]}$ can be implemented into $\\theta$  \n",
    "and all the $dW^{[1]} db^{[1]}....dW^{[L]} db^{[L]}$ can be implemented into $d\\theta$  \n",
    "$d\\theta(i)$ can be described as:  \n",
    "$d\\theta(i) = \\frac{d(\\theta1,\\theta2,...\\theta(i+\\epsilon))-d(\\theta1,\\theta2,...\\theta(i-\\epsilon))}{2\\epsilon}$  \n",
    "then we get $d\\theta_{appox}$ compared to $d\\theta$  \n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$  \n",
    "difference should small than one number for example $10^{-7}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
