{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. hyperparameters tuning importance\n",
    "\n",
    "list by the importance\n",
    "1. $ \\alpha $  \n",
    "2. hidden unit, mini-batch size, $\\beta$ etc  \n",
    "3. $\\beta1(0.9),\\beta2(0.999), \\epsilon(10^{-8})$, layers, learning_rate decay\n",
    "\n",
    "1.randomly search the hyperparameters not by the grid\n",
    "2. select the appropriate scale of your hyperparameters [0.0001-0.1] can't select linearly transform oto [-3,-1]\n",
    "\n",
    "### 2. batch normalization\n",
    "to let your hyperparameters' scale more\n",
    "\n",
    "method:  \n",
    "in the forward propagation, substitute $z^{[L]}$ with $z^{*[L]}$（norm the $z^{[L]}$)  \n",
    "$\\mu = 1/m \\sum_{i = 1}^mz^{[L]}$  \n",
    "$\\sigma = 1/m \\sum_{i = 1}^m(z^{[L]}-\\mu)^2$  \n",
    "$z_{norm}^{[L]} = \\frac{z^{[L]}-\\mu}{\\sigma}$  \n",
    "$z^{*[L]} = \\gamma z_{norm}^{[L]} + \\beta$\n",
    "to let $z^{*[L]}$ have the solid mean and variance  \n",
    "<img src=\"batch.PNG\" width = 40%>\n",
    "**note: $\\gamma and \\beta$ are the learning parameters, which needed to be updated using gradient descend!**\n",
    "\n",
    "implement:\n",
    "\n",
    "<img src = \"batch2.PNG\" width = 60%>\n",
    "why does it work?  \n",
    "1.norm every layer's input to speed up the program as norm the input and it works on mini-batch!!!\n",
    "2. have some regularization effect\n",
    "\n",
    "when testing:(not in the mini-batch)\n",
    "using the $\\mu$ and $\\sigma$  that tracked in the training set using exponentially average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### muti-classifier\n",
    "\n",
    "softmax regression --- generized the logistic regression to C classifier\n",
    "\n",
    "$t = e^{z^{[L]}}$  \n",
    "$a^{[L]} = \\frac{e^{z^{[L]}}}{\\sum t_i}$\n",
    "\n",
    "\n",
    "### exercise\n",
    "<img src = \"parameter_scale.PNG\" width=\"40%\">\n",
    "\n",
    "### procedure that tensorflow does\n",
    "Writing and running programs in TensorFlow has the following steps:\n",
    "\n",
    "1. Create Tensors (variables) that are not yet executed/evaluated. \n",
    "2. Write operations between those Tensors.\n",
    "3. Initialize your Tensors. \n",
    "4. Create a Session. \n",
    "5. Run the Session. This will run the operations you'd written above. \n",
    "\n",
    "Note that there are two typical ways to create and use sessions in tensorflow: \n",
    "\n",
    "**Method 1:**\n",
    "```python\n",
    "sess = tf.Session()\n",
    "# Run the variables initialization (if needed), run the operations\n",
    "result = sess.run(..., feed_dict = {...})\n",
    "sess.close() # Close the session\n",
    "```\n",
    "**Method 2:**\n",
    "```python\n",
    "with tf.Session() as sess: \n",
    "    # run the variables initialization (if needed), run the operations\n",
    "    result = sess.run(..., feed_dict = {...})\n",
    "    # This takes care of closing the session for you :)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均值为：3.500000\n",
      "方差为：2.916667\n",
      "标准差为:1.870829\n"
     ]
    }
   ],
   "source": [
    "### verify how to calculate the means and variance using python\n",
    "### example\n",
    "import numpy as np \n",
    "arr = [1,2,3,4,5,6]\n",
    "#求均值\n",
    "arr_mean = np.mean(arr)\n",
    "#求方差\n",
    "arr_var = np.var(arr)\n",
    "#求标准差\n",
    "arr_std = np.std(arr,ddof=1)\n",
    "print(\"平均值为：%f\" % arr_mean)\n",
    "print(\"方差为：%f\" % arr_var)\n",
    "print(\"标准差为:%f\" % arr_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of x: 0.39421744135916204\n",
      "variance of x: 0.24446628230673734\n",
      "mean of y: 3.182652324077486\n",
      "variance of y: 2.200196540760636\n",
      "mean of y: 1.9855844366986728\n",
      "standard variance of y: 3.519104223030111\n"
     ]
    }
   ],
   "source": [
    "### how the y = ax+b change the mean and variance ?\n",
    "import numpy as np\n",
    "x = np.random.randn(10)\n",
    "print(\"mean of x: {}\".format(np.mean(x)))\n",
    "print(\"variance of x: {}\".format(np.var(x)))\n",
    "\n",
    "y = 3*x + 2\n",
    "\n",
    "print(\"mean of y: {}\".format(np.mean(y)))\n",
    "print(\"variance of y: {}\".format(np.var(y)))\n",
    "\n",
    "###normal才能创建具有具体标准差和均值的数组\n",
    "z = np.random.normal(loc=0, scale=1, size=100)\n",
    "y = 3*z + 2\n",
    "\n",
    "print(\"mean of y: {}\".format(np.mean(y)))\n",
    "print(\"standard variance of y: {}\".format(np.std(y)))\n",
    "\n",
    "### y = a*x + b will move the mean value of x plus b ,and the std variance of x multiply by a \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
