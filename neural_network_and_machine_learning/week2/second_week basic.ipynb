{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.logistic regression model\n",
    "\n",
    "to solve the binary problem \n",
    "2 parameter: w and b w--- nx dimensional metrix b a real number\n",
    "sigmoid function:  \n",
    "to let the result between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"sigmoid\" width =30%>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"sigmoid\" width =30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.loss function\n",
    "the function of y(hat) and y, actual a function of w and b  \n",
    "$L(w,b) = -(ylog^{y^p}+(1-y)log^(1-y^p))$\n",
    "m的公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. gradient decend algorithm\n",
    "gradient decend function: to find w and b to minimize the L(w,b)\n",
    "and the L(w,b) has only one smallest value\n",
    "the process is like this picture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"gradient decend.PNG\" width = 40%>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"gradient decend.PNG\" width = 40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we have the initial value of w1 and b1  \n",
    "and then we use :  \n",
    "$ w = w - \\alpha\\frac{dL}{dw}$  \n",
    "$ b = b - \\alpha\\frac{dL}{db}$\n",
    "in this way, we call \n",
    "$\\frac{dL}{dw}= dw $  \n",
    "$\\frac{dL}{db}= db $  \n",
    "so  \n",
    "$ w = w - \\alpha dw$  \n",
    "$ b = b - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. derivative regression\n",
    "graph:(for one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"derivative graph.PNG\" width=60%>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"derivative graph.PNG\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note:$\\frac{da}{dz} = a(1-a)$ ---important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. for m example\n",
    "two loop need to implement , using vectorization to avoid those loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization20.946502685546875ms\n",
      "for loop17601.714611053467ms\n"
     ]
    }
   ],
   "source": [
    "# demonstrate the vectorization function\n",
    "import numpy as np\n",
    "import time \n",
    "a = np.random.rand(100000)\n",
    "b = np.random.rand(100000)\n",
    "## for vectorization\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(\"vectorization\"+str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "## for loop version\n",
    "c= 0\n",
    "tic = time.time()\n",
    "for i in range(100000):\n",
    "    c += a*b\n",
    "toc = time.time()\n",
    "print(\"for loop\"+str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 5]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1,2,-1,5])\n",
    "p = np.maximum(v,0)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the logistic regression vectorization, we have:\n",
    "$Z = W^Tx+b$  \n",
    "$A = \\sigma(Z)$  \n",
    "$dZ = A - Y$  \n",
    "$dW = \\frac1mXdZ^T$  \n",
    "$db = \\frac1m.sum(dZ)$\n",
    "## note : the way to write vectorization : firstly write the for loop version and write the right metrix number of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. broadcast\n",
    "\n",
    "(m,n) + (1,n) = (m,n) + (m,n)\n",
    "\n",
    "and avoid the shape of (5,)(rank1 array)\n",
    "if you implment np.random.randn(5) will get (5,)\n",
    "use np.random.randn(5,1)\n",
    "or assert a.shape == (5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.assignment note!\n",
    "in python a*b is the element-wise multiply\n",
    "and np.dot(a,b) is the matrix multiply\n",
    "element-wise mutiply will invoke the broadcast!\n",
    "and broadcast need the shape to be (n,1) or (1,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58327086  1.47147438 -0.89119082  0.22317557 -0.61635017]\n",
      " [ 0.12380023  0.17530099  0.02810227 -0.09374038 -0.34121751]\n",
      " [ 0.93287692  1.04678902 -0.36533111 -0.87654886  0.51925539]\n",
      " [-3.97165831 -0.1687905  -0.7866699  -0.34498805 -1.31529451]]\n"
     ]
    }
   ],
   "source": [
    "###example:\n",
    "a = np.random.randn(4,5)\n",
    "b = np.random.randn(4,1)\n",
    "\n",
    "c = a*b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matrixproduct() missing required argument 'b' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bbaac2511206>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: matrixproduct() missing required argument 'b' (pos 2)"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(4,5)\n",
    "b = np.random.randn(4,1)\n",
    "\n",
    "c = np.dot(a*b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
